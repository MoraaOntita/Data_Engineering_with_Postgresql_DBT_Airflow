{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 13811 fields in line 3, saw 87611\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 13811 fields in line 3, saw 87611\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows/lines is 6170\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of rows/lines is {len(lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id; type; traveled_d; avg_speed; lat; lon; speed; lon_acc; lat_acc; time\n",
      "\n",
      "['track_id', ' type', ' traveled_d', ' avg_speed', ' lat', ' lon', ' speed', ' lon_acc', ' lat_acc', ' time']\n"
     ]
    }
   ],
   "source": [
    "print(lines[0]) # column names\n",
    "print(lines[0].strip('\\n').strip().strip(';').split(';')) # columns names as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: track_id; type; traveled_d; avg_speed; lat; lon; speed; lon_acc; lat_acc; time\n",
      "Total number of lines: 6170\n"
     ]
    }
   ],
   "source": [
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    line_count = 0\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "        if line_count == 1:\n",
    "            print(\"Column names:\", line.strip())\n",
    "        else:\n",
    "            # Process other lines here if needed\n",
    "            pass\n",
    "\n",
    "print(\"Total number of lines:\", line_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: track_id; type; traveled_d; avg_speed; lat; lon; speed; lon_acc; lat_acc; time\n",
      "The number of fields in row 1 is 13811\n",
      "The number of fields in row 2 is 87611\n",
      "Total number of lines: 6170\n"
     ]
    }
   ],
   "source": [
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    line_count = 0\n",
    "    for line in file:\n",
    "        line_count += 1\n",
    "        if line_count == 1:\n",
    "            print(\"Column names:\", line.strip())\n",
    "        else:\n",
    "            if line_count == 2:\n",
    "                fields = line.strip().split(';')\n",
    "                print(f\"The number of fields in row 1 is {len(fields)}\")\n",
    "            elif line_count == 3:\n",
    "                fields = line.strip().split(';')\n",
    "                print(f\"The number of fields in row 2 is {len(fields)}\")\n",
    "            # Continue processing other lines if needed\n",
    "            pass\n",
    "\n",
    "print(\"Total number of lines:\", line_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of fields is 136073\n",
      "The largest n = 22678\n"
     ]
    }
   ],
   "source": [
    "no_field_max = 0\n",
    "\n",
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    for line in file:\n",
    "        fields = line.strip().split(';')\n",
    "        if len(fields) > no_field_max:\n",
    "            no_field_max = len(fields)\n",
    "\n",
    "print(f\"The maximum number of fields is {no_field_max}\")\n",
    "largest_n = int((no_field_max - 4) / 6)\n",
    "print(f\"The largest n = {largest_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store column names\n",
    "cols = []\n",
    "\n",
    "# Open the CSV file and read it line by line\n",
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    for line_num, line in enumerate(file):\n",
    "        # Split the line into fields using the specified delimiter (;)\n",
    "        fields = line.strip().split(';')\n",
    "        # If it's the first line, store column names\n",
    "        if line_num == 0:\n",
    "            cols = fields\n",
    "        # Otherwise, process the data as needed\n",
    "        else:\n",
    "            # Process data here if necessary\n",
    "            pass\n",
    "\n",
    "# Remove the first element (column names) from cols\n",
    "if cols:\n",
    "    cols.pop(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['track_id', ' type', ' traveled_d', ' avg_speed']\n",
      "['track_id', ' lat', ' lon', ' speed', ' lon_acc', ' lat_acc', ' time']\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store track_cols and trajectory_cols\n",
    "track_cols = []\n",
    "trajectory_cols = []\n",
    "\n",
    "# Open the CSV file and read it line by line\n",
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    for line_num, line in enumerate(file):\n",
    "        # Split the line into fields using the specified delimiter (;)\n",
    "        fields = line.strip().split(';')\n",
    "        # If it's the first line, determine track_cols and trajectory_cols\n",
    "        if line_num == 0:\n",
    "            track_cols = fields[:4]\n",
    "            trajectory_cols = ['track_id'] + fields[4:]\n",
    "            break  # No need to continue processing lines\n",
    "        # Stop processing after the first line to save memory\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(track_cols)\n",
    "print(trajectory_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store track_info and trajectory_info\n",
    "track_info = []\n",
    "trajectory_info = []\n",
    "\n",
    "# Open the CSV file and read it line by line\n",
    "with open(\"/media/moraa/New Volume/Ontita/10Academy/Cohort B/Projects/Week2/Data_Engineering_with_Postgresql_DBT_Airflow/Data/downloaded/20181101_dX_0900_0930.csv\", 'r') as file:\n",
    "    # Skip the header line\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        # Split the line into fields using the specified delimiter (;)\n",
    "        fields = line.strip().split(';')\n",
    "        track_id = fields[0]\n",
    "        # Append the first 4 values to track_info\n",
    "        track_info.append(fields[:4])\n",
    "        # Process the remaining values\n",
    "        remaining_values = fields[4:]\n",
    "        # Reshape the remaining values into chunks of length 6\n",
    "        chunk_size = 6\n",
    "        for i in range(0, len(remaining_values), chunk_size):\n",
    "            trajectory_info.append([track_id] + remaining_values[i:i+chunk_size])\n",
    "\n",
    "# Print first few elements of track_info and trajectory_info\n",
    "print(track_info[:5])\n",
    "print(trajectory_info[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
